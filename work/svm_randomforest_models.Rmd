---
title: "models_svm_randomforest"
author: "Sarah"
date: "11/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggspatial)
library(maptools)
library(tigris)
library(caret)
library(corrplot)
```

```{r}
# load data
pixels_data <- read.csv('pixels_data.csv')

# group by number of polygons per tree class per strip
poly_count <- pixels_data %>%
  group_by("Raster Strip" = rstrip, "Tree Name" = Cmmn_Nm) %>%
  count("Polygons" = length(unique(ID)))

# split into 70% train, 30% test, stratified on tree classes per raster strip

# training data
train <- pixels_data %>%
  group_by(rstrip, Cmmn_Nm) %>%
  slice_sample(prop = 0.7) %>%
  ungroup()

# check 70% in each class
train %>%
  group_by("Raster Strip" = rstrip, "Tree Name" = Cmmn_Nm) %>%
  count("Polygons" = length(unique(ID)))

# test data
test_full <- anti_join(pixels_data, train)

# adding different ratio predictors
train <- train %>%
  dplyr::select(red, green, blue, ir, ndvi, Cmmn_Nm) %>%
  mutate(red_blue = red/blue,
         ir_red = ir/red,
         red_green = red/green,
         blue_green = blue/green)

test <- test_full %>%
  dplyr::select(red, green, blue, ir, ndvi, Cmmn_Nm) %>%
  mutate(red_blue = red/blue,
         ir_red = ir/red,
         red_green = red/green,
         blue_green = blue/green)
```

```{r}
# preprocess pca
train_pca <- preProcess(dplyr::select(train, -Cmmn_Nm), method = c("center", "scale", "pca"))
train_pca
train_pca$method
train_pca$rotation
```

```{r}
# look at correlation matrix
corr_matrix <- round(cor(train %>% select(-Cmmn_Nm)), 2)
corrplot(corr_matrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

# ir_red not as highly correlated as other ratio predictors, but still highly correlated with ndvi
# red_green strongest correlation with green at -0.81
# red_blue strongest correlation with blue

train1 <- train %>%
  select(red, green, blue, ir, ndvi, Cmmn_Nm)

train2 <- train %>%
  select(red, green, blue, ir, ndvi, red_green, blue_green, Cmmn_Nm)

train3 <- train %>%
  select(red, green, blue, ir, ndvi, red_green, red_blue, blue_green, Cmmn_Nm)

# random forest

# random search random forest using caret package
control <- trainControl(method = "repeatedcv", number = 10, search = "random")
rf_random1 <- train(Cmmn_Nm~., data = train1, method = "rf", trControl = control)
rf_random2 <- train(Cmmn_Nm~., data = train2, method = "rf", trControl = control)
rf_random3 <- train(Cmmn_Nm~., data = train3, method = "rf", trControl = control)

#predict(test, rf_random)
```

```{r}
# random forest mtry tuning

# Random Search
control <- trainControl(method="repeatedcv", number = 10, repeats = 3, search = "random")
set.seed(12)
mtry <- sqrt(ncol(train3))
rf_random <- train(Cmmn_Nm~., data = train3, method = "rf", tuneLength = 15,
                   trControl = control)
print(rf_random)
plot(rf_random)
confusionMatrix(rf_random,  mode = "prec_recall")
# rf_random western redcedar prediction accuracy: 0.36
```

```{r}
# compare rf models

# collect resamples
results <- rbind(cbind(rf_random1$resample, Model = rep("5", nrow(rf_random1$resample))), 
                 cbind(rf_random2$resample, Model = rep("7", nrow(rf_random2$resample))), 
                 cbind(rf_random3$resample, Model = rep("8", nrow(rf_random3$resample))))

ggplot(results, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() + 
  scale_fill_viridis_d() +
  labs(title = "Model Accuracy of Random Forest Models with 10-Fold CV", x = " ", fill = "Number of Predictors") + 
  theme(axis.text.x = element_blank())

confusionMatrix(rf_random3,  mode = "prec_recall")
rf_random1[["finalModel"]]
rf_random2[["finalModel"]]
rf_random3[["finalModel"]]
varImp(rf_random1)
varImp(rf_random2)
varImp(rf_random3)
# rf_random1 western redcedar prediction accuracy: 0.33
# rf_random2 western redcedar prediction accuracy: 0.37
# rf_random3 western redcedar prediction accuracy: 0.37

results_rf <- rbind(cbind(rf_random1$resample, Model = rep("5", 10), 
                          mtry = rep(rf_random1$bestTune[1,1], 10)),
                    cbind(rf_random2$resample, Model = rep("7", 10),
                          mtry = rep(rf_random2$bestTune[1,1], 10)),
                    cbind(rf_random3$resample, Model = rep("8", 10),
                          mtry = rep(rf_random3$bestTune[1,1], 10)))
```


```{r}
train <- train %>%
  mutate(Cmmn_Nm = make.names(Cmmn_Nm))

# 10 fold cross validation
train_control <- trainControl(method = "cv", number = 10, classProbs = T)

# fit svm model with normalized variables
svm1 <- train(Cmmn_Nm ~., data = train, method = "svmLinear", trControl = train_control,  preProcess = c("center","scale"))

# fit nonlinear svm model (radial basis)
svm2 <- train(Cmmn_Nm ~., data = train, method = "svmRadial", trControl = train_control, preProcess = c("center","scale"), tuneLength = 10)

# fit nonlinear svm model (polynomial basis)
svm3 <- train(Cmmn_Nm ~., data = train, method = "svmPoly", trControl = train_control, preProcess = c("center","scale"))
```

```{r}
# compare svm models

# collect resamples
results_svm <- rbind(cbind(svm1$resample, Model = rep("svmLinear", nrow(svm1$resample), 10)),
                     cbind(svm2$resample, Model = rep("svmRadial", nrow(svm2$resample), 10)),
                     cbind(svm3$resample, Model = rep("svmPoly", nrow(svm3$resample), 10)))

ggplot(results_svm, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() + 
  scale_fill_viridis_d() +
  labs(title = "Model Accuracy of SVM Models with 10-Fold CV", x = " ", fill = " ") + 
  theme(axis.text.x = element_blank())

confusionMatrix(svm1,  mode = "prec_recall")
# svm1 western redcedar prediction accuracy: 0.32
# svm2 western redcedar prediction accuracy: 0.41
# svm3 western redcedar prediction accuracy: 0.34
```

```{r}
# combined hyperparameters results table
results_r <- results_rf %>%
  mutate(Model = recode(as.character(Model), '5'= '5 predictors',
                        '7' = '7 predictors', '8' = '8 predictors')) %>%
  group_by(Model) %>%
  summarise("Accuracy" = round(mean(Accuracy), 4), "Kappa" = round(mean(Kappa), 4),
            "mtry" = mean(mtry)) %>%
  rename("Random Forest Model" = Model)

results_s <- results_svm %>%
  mutate(Model = recode(as.character(Model), 'svmLinear'= 'Linear',
                        'svmRadial' = 'Radial', 'svmPoly' = 'Polynomial')) %>%
  group_by(Model) %>%
  summarise("Accuracy" = round(mean(Accuracy), 4), "Kappa" = round(mean(Kappa), 4)) %>%
  rename("SVM Model Kernel" = Model)

results_s$Parameter <- c(paste("C =", svm1$bestTune[1,1]), 
                         paste("C =", svm2$bestTune[1,2], 
                               "sigma =", round(svm2$bestTune[1,1], 4)), 
                         paste("C =", svm3$bestTune[1,3], 
                               ", scale =", svm3$bestTune[1,2], 
                               ", degree =", svm3$bestTune[1,1]))

# save results
# write.csv(results_r,'results_r.csv', row.names = F)
# write.csv(results_s,'results_s.csv', row.names = F)
```

```{r}
# test set results for overall prediction
test_results <- data.frame(Class = test$Cmmn_Nm)
test_results$RF <- predict(rf_random3, test)
test_results$SVM <- predict(svm2, test)

test_results %>%
  summarise("RF Accuracy" = sum(Class == RF)/nrow(test_results), 
            "SVM Accuracy" = sum(Class == SVM)/nrow(test_results))

# test set results for predicting Western Redcedar
test_results_redceder <- test_results %>%
  filter(Class %in% "Western Redcedar")

test_results_redceder %>%
  summarise(rf_accuracy = sum(Class == RF)/nrow(test_results), 
            svm_accuracy = sum(Class == SVM)/nrow(test_results))

# overall test set results for polygons
# join test results to full test data (with polygon info)
test_dat <- cbind(test_full, test_results[,-1]) 

# "correct" prediction if more than half pixels in polygon are correctly predicted

# rf polygon results
poly_test_rf <- test_dat %>%
  group_by(ID) %>%
  count(ID, same_rf = (RF == Cmmn_Nm), total = n()) %>%
  pivot_wider(names_from = same_rf, values_from = n)

poly_test_rf[is.na(poly_test_rf)] <- 0 # replace na with 0

poly_test_rf <- poly_test_rf %>%
  mutate(result = case_when(`TRUE` > `FALSE` ~ "correct",
                                   `TRUE` <= `FALSE` ~ "incorrect"))

poly_test_rf %>%
  group_by(result) %>%
  count(result)

# svm polygon results
poly_test_svm <- test_dat %>%
  group_by(ID) %>%
  count(ID, same_svm = (SVM == Cmmn_Nm), total = n()) %>%
  pivot_wider(names_from = same_svm, values_from = n)

poly_test_svm[is.na(poly_test_svm)] <- 0 # replace na with 0

poly_test_svm <- poly_test_svm %>%
  mutate(result = case_when(`TRUE` > `FALSE` ~ "correct",
                                   `TRUE` <= `FALSE` ~ "incorrect"))

poly_test_svm %>%
  group_by(result) %>%
  count(result)

# save results
# write.csv(test_results,'test_results.csv', row.names = F)
# write.csv(poly_test_rf,'poly_test_rf.csv', row.names = F)
# write.csv(poly_test_svm,'poly_test_svm.csv', row.names = F)
```

Plot results
```{r}
# combine with latitude info from poly_reprojected in work/geographic_join.Rmd
poly_join_reprojected@data <- poly_join_reprojected@data %>%
  left_join(poly_test_vector, by = c("id" = "ID"))

# make key to match projected data with vector using spCbind
o <- match(poly_join_reprojected@data$id, poly_test_vector$ID)
results_vector <- poly_test_vector[o,]
spCbind(obj = poly_join_reprojected, x = results_vector)

# make dataframe for ggplot
results_pts <- fortify(poly_join_reprojected, region = "id")
results_df <- merge(results_pts, poly_join_reprojected@data, by = "id")

# initial ggplot
ggplot(results_df, aes(x = long, y = lat, group = id, color = result)) + 
  geom_polygon(size = 2) +
  scale_fill_brewer() + theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none")

# add city outline
mult_county <- county_subdivisions('Oregon', 'Multnomah')

poly_sf <- st_as_sf(poly_join_reprojected)

ggplot(data = poly_sf, aes(color = result)) + 
  geom_sf(size = 2) +
  scale_color_viridis_d(direction = -1)
```

```{r}
# boosted tree using caret package
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           repeats = 3)
set.seed(825)
gbmFit1 <- train(Cmmn_Nm ~ ., data = train, 
                 method = "gbm", 
                 trControl = fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)
gbmFit1
```
